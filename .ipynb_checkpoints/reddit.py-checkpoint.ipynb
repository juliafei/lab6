{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.67.71:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS = \"reddit.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/cloudpickle.py\", line 235, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 224, in dump\n",
      "    self.save(obj)\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 568, in save_tuple\n",
      "    save(element)\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/cloudpickle.py\", line 378, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/cloudpickle.py\", line 529, in save_function_tuple\n",
      "    save(closure_values)\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 642, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/Users/julialau/anaconda/lib/python2.7/pickle.py\", line 306, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 220, in __getnewargs__\n",
      "    \"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\n",
      "Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/julialau/anaconda/lib/python2.7/site-packages/IPython/core/formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/anaconda/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    396\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/anaconda/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/anaconda/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getnewargs__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2470\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2471\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2472\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2401\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2404\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2405\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/cloudpickle.pyc\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m     \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/cloudpickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(COMMENTS, use_unicode=False).cache()\n",
    "list(enumerate(rdd.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"score\":-1,\"ups\":-1,\"created_utc\":1157068927,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fr9p\",\"body\":\"This is untested territory legally - I can\\'t wait to see what happens to the first entity who exploits someone\\'s uploaded work in a way the creator doesn\\'t like and/or makes loads of money from it and doesn\\'t pay the creator anything. To give them the benefit of the doubt, I think most \\\\\"sharing\\\\\" site included scary-sounding ToS to protect themselves from some uploader being a jerk: \\\\\"Hey, there\\'s an ad on the same page as my stuff, I never agreed to that, I\\'m suing you!\\\\\"\\\\r\\\\n\\\\r\\\\nArguably, these sites have \\\\\"paid\\\\\" their contributors by doing unpaid promotion work and providing hosting.  Anyone have any idea how much it would cost an individual to host and promote a video such that it reaches millions of people?  I think one of these sites will cross a line - e.g. put out a CD/DVD that makes millions and pay nothing to the contributors.  Then we\\'ll see what happens as lawyers see an opportunity to chase big bucks through the courts.\",\"controversiality\":0,\"distinguished\":null,\"retrieved_on\":1473797859,\"gilded\":0,\"id\":\"cfuqz\",\"edited\":false,\"parent_id\":\"t3_fr9p\",\"author_flair_text\":null,\"author\":\"Anderkay\"}', '{\"body\":\"[deleted]\",\"controversiality\":1,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_ft89\",\"subreddit\":\"reddit.com\",\"score\":2,\"ups\":2,\"created_utc\":1157068946,\"author_flair_css_class\":null,\"author\":\"[deleted]\",\"author_flair_text\":null,\"id\":\"cfur0\",\"edited\":false,\"parent_id\":\"t1_cfuqg\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797859}', '{\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_frwf\",\"stickied\":false,\"controversiality\":1,\"body\":\"[removed]\",\"ups\":-3,\"score\":-3,\"author_flair_css_class\":null,\"created_utc\":1157068993,\"subreddit\":\"reddit.com\",\"id\":\"cfur8\",\"edited\":false,\"parent_id\":\"t1_cfuml\",\"author\":\"[deleted]\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797859,\"gilded\":0}', '{\"body\":\"Not anymore.  Not sure when they started, but now they routinely marry Amish from other areas of the US to prevent this.\\\\r\\\\n\\\\r\\\\nI used to see much evidence of inbreeding, retardation, club feet, etc, when I would visit Amish communities 30 years ago, but I haven\\'t seen anything like that for years.\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_ft89\",\"stickied\":false,\"subreddit\":\"reddit.com\",\"score\":6,\"ups\":6,\"author_flair_css_class\":null,\"created_utc\":1157069061,\"author\":\"Smiley\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfurb\",\"parent_id\":\"t1_cfthp\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797859}', '{\"gilded\":0,\"retrieved_on\":1473797859,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"[deleted]\",\"edited\":false,\"parent_id\":\"t1_cfud7\",\"id\":\"cfure\",\"subreddit\":\"programming\",\"author_flair_css_class\":null,\"created_utc\":1157069151,\"score\":5,\"ups\":5,\"controversiality\":1,\"body\":\"&gt; If you\\'re wishing for other good circa-1980s technology, the\\\\r\\\\n&gt; development tools in Squeak blow away Eclipse, Emacs (sad to &gt; say) et al.\\\\r\\\\n\\\\r\\\\nThat\\'s probably why there\\'s so much *awesome* software written in Smalltalk.\\\\r\\\\n\",\"link_id\":\"t3_frq4\",\"stickied\":false,\"subreddit_id\":\"t5_2fwo\"}', '{\"stickied\":false,\"link_id\":\"t3_ftam\",\"subreddit_id\":\"t5_6\",\"body\":\"I\\'m getting a bit sick and tired of idiots modding based on whether or not they like the conclusions.\\\\r\\\\n\\\\r\\\\nWe should open \\\\\"wankery.reddit.com\\\\\", where people can go and submit articles they agree with and then nod vigorously at each other.\",\"controversiality\":0,\"created_utc\":1157069285,\"author_flair_css_class\":null,\"score\":12,\"ups\":12,\"subreddit\":\"reddit.com\",\"edited\":false,\"id\":\"cfuri\",\"parent_id\":\"t1_cfubs\",\"author_flair_text\":null,\"author\":\"mikepurvis\",\"retrieved_on\":1473797859,\"distinguished\":null,\"gilded\":0}', '{\"author\":\"[deleted]\",\"author_flair_text\":null,\"id\":\"cfurj\",\"parent_id\":\"t1_cfsv3\",\"edited\":false,\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797859,\"body\":\"He was probably to stubborn to turn back... that would be a bit embarassing...\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fra8\",\"subreddit\":\"reddit.com\",\"score\":5,\"ups\":5,\"created_utc\":1157069302,\"author_flair_css_class\":null}', '{\"stickied\":false,\"link_id\":\"t3_fuam\",\"subreddit_id\":\"t5_2fwo\",\"controversiality\":0,\"body\":\"It\\'s not clearly apocryphal at all. \\\\r\\\\n\\\\r\\\\nIs it that much of a stretch that this guy has a wife who, while not necissarily a techie, can ask valid questions about a technical subject, that he and his wife can hold basic conversations? \\\\r\\\\n\\\\r\\\\nMy girlfriend and I often do this sort of thing and she\\'s an linguistic anthropologist; certainly not a computer geek.\",\"created_utc\":1157069314,\"author_flair_css_class\":null,\"score\":5,\"ups\":5,\"subreddit\":\"programming\",\"id\":\"cfurk\",\"edited\":false,\"parent_id\":\"t1_cfuq5\",\"author_flair_text\":null,\"author\":\"[deleted]\",\"retrieved_on\":1473797859,\"distinguished\":null,\"gilded\":0}', '{\"body\":\"There\\'s something incredibly perverse about replacing a loved one with a foam cut-out...that can\\'t be healthy...\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fuqi\",\"subreddit\":\"reddit.com\",\"score\":2,\"ups\":2,\"created_utc\":1157069566,\"author_flair_css_class\":null,\"author\":\"PatternJuggler\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfusd\",\"parent_id\":\"t3_fuqi\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860}', '{\"author_flair_css_class\":null,\"created_utc\":1157069616,\"score\":12,\"ups\":12,\"subreddit\":\"reddit.com\",\"link_id\":\"t3_fr8b\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"It has a hand crank.  Chuck Norris is the only person capable of starting it.\",\"retrieved_on\":1473797860,\"distinguished\":null,\"gilded\":0,\"edited\":false,\"id\":\"cfusj\",\"parent_id\":\"t1_cfu4o\",\"author_flair_text\":null,\"author\":\"kg2\"}', '{\"author_flair_text\":null,\"author\":\"notfancy\",\"edited\":false,\"id\":\"cfusn\",\"parent_id\":\"t1_cftop\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"controversiality\":0,\"body\":\"Shouldn\\'t that be \\\\\"There\\'s no kind of people\\\\\"? It sounds to me like a restatement of Russel\\'s Paradox.\\\\r\\\\n\\\\r\\\\nI just came up with: \\\\\"There\\'s one kind of people in the world: those who are referenced by this very sentence, and those who aren\\'t.\\\\\"\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_frco\",\"subreddit\":\"reddit.com\",\"ups\":1,\"score\":1,\"created_utc\":1157069731,\"author_flair_css_class\":null}', '{\"ups\":3,\"score\":3,\"created_utc\":1157069796,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fq6q\",\"body\":\"Personally, I wish that the population of every country, not just America, was exposed to critical and potent journalism all the time. \\\\r\\\\n\\\\r\\\\nIt doesn\\'t have to be right. It just has to show that thinking is allowed, dissent is encouraged, and free speech means something.\",\"controversiality\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"gilded\":0,\"id\":\"cfusq\",\"edited\":false,\"parent_id\":\"t1_cfrxj\",\"author_flair_text\":null,\"author\":\"Random\"}', '{\"link_id\":\"t3_fra8\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"It\\'s not hate.  It\\'s practicing what you preach.  Talk is cheap, especially in religion.\",\"author_flair_css_class\":null,\"created_utc\":1157069903,\"score\":4,\"ups\":4,\"subreddit\":\"reddit.com\",\"parent_id\":\"t1_cfsi2\",\"edited\":false,\"id\":\"cfusu\",\"author_flair_text\":null,\"author\":\"microfish\",\"retrieved_on\":1473797860,\"distinguished\":null,\"gilded\":0}', '{\"link_id\":\"t3_frv2\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"maybe you should have asked that question when they made child porn illegal.\",\"author_flair_css_class\":null,\"created_utc\":1157070037,\"score\":2,\"ups\":2,\"subreddit\":\"reddit.com\",\"edited\":false,\"id\":\"cfusz\",\"parent_id\":\"t1_cfspf\",\"author\":\"vipstar\",\"author_flair_text\":null,\"retrieved_on\":1473797860,\"distinguished\":null,\"gilded\":0}', '{\"author_flair_text\":null,\"author\":\"nzeeshan\",\"parent_id\":\"t3_fut5\",\"edited\":false,\"id\":\"cfuta\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"body\":\"Hiding one\\'s shadow during a picture is very important when it comes to photography .. however in this case .. the beauty is in the shadow .. the photographer\\'s shadow tells what stance he stood as he took the picture .. the photographer\\'s shadow is a unique style (unintended though)\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fut5\",\"subreddit\":\"reddit.com\",\"score\":2,\"ups\":2,\"created_utc\":1157070092,\"author_flair_css_class\":null}', '{\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fnbe\",\"stickied\":false,\"controversiality\":0,\"body\":\"[deleted]\",\"ups\":1,\"score\":1,\"author_flair_css_class\":null,\"created_utc\":1157070134,\"subreddit\":\"reddit.com\",\"parent_id\":\"t1_cfoky\",\"edited\":false,\"id\":\"cfutf\",\"author\":\"[deleted]\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797860,\"gilded\":0}', '{\"body\":\"The Girl In The Caf\\\\u00e9 took home three accolades at 58th Annual Prime Time Emmy Awards including the prestigious Outstanding Made For Television Movie award.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_futg\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157070174,\"author_flair_css_class\":null,\"score\":1,\"ups\":1,\"author\":\"pnk_master\",\"author_flair_text\":null,\"id\":\"cfuth\",\"parent_id\":\"t3_futg\",\"edited\":false,\"gilded\":0,\"retrieved_on\":1473797860,\"distinguished\":null}', '{\"author_flair_text\":null,\"author\":\"nzeeshan\",\"id\":\"cfutm\",\"parent_id\":\"t3_futk\",\"edited\":false,\"gilded\":0,\"retrieved_on\":1473797860,\"distinguished\":null,\"body\":\"Powers of 10 is a film dealing with the relative size of things in the Universe, and the effect of adding another zero. The film was made for IBM by the offices of Charles and Ray Eames\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_futk\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157070212,\"author_flair_css_class\":null,\"score\":1,\"ups\":1}', '{\"id\":\"cfuto\",\"parent_id\":\"t3_futl\",\"edited\":false,\"author_flair_text\":null,\"author\":\"sr4001\",\"retrieved_on\":1473797860,\"distinguished\":null,\"gilded\":0,\"stickied\":false,\"link_id\":\"t3_futl\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"Combine the story from earlier today of the polygamists in Utah and the 11 yr old condemned for having an abortion by the church and you get a picture of what this child bride\\'s future would be like.\",\"created_utc\":1157070282,\"author_flair_css_class\":null,\"score\":2,\"ups\":2,\"subreddit\":\"reddit.com\"}', '{\"author\":\"nzeeshan\",\"author_flair_text\":null,\"id\":\"cfuts\",\"parent_id\":\"t3_futr\",\"edited\":false,\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"body\":\"There\\'s a common misconception among people that everything looks black and white to colorblind people .. well it\\'s not true (only in total color blindness you see black and white). There are many different types of degress when it comes to color deficiency .. following images show you how colors change to a color blind person ..\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_futr\",\"subreddit\":\"reddit.com\",\"score\":0,\"ups\":0,\"created_utc\":1157070345,\"author_flair_css_class\":null}', '{\"subreddit\":\"reddit.com\",\"score\":1,\"ups\":1,\"author_flair_css_class\":null,\"created_utc\":1157070361,\"body\":\"First Britain is the largest terrorist threat to the US and now the Canadians are arming their border guards! What next?\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_futq\",\"stickied\":false,\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"author\":\"sr4001\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfutv\",\"parent_id\":\"t3_futq\"}', '{\"author_flair_text\":null,\"author\":\"axodys\",\"edited\":false,\"parent_id\":\"t3_frpe\",\"id\":\"cfutw\",\"gilded\":0,\"retrieved_on\":1473797860,\"distinguished\":null,\"body\":\"That\\'s pretty wild from an American perspective.  It sounds somewhat similar to England a few hundred years back though.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_frpe\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157070405,\"author_flair_css_class\":null,\"ups\":1,\"score\":1}', '{\"score\":1,\"ups\":1,\"created_utc\":1157070440,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fsgf\",\"controversiality\":0,\"body\":\"The high oil prices which the Western world abhors are a product of Western style capitalism at it\\'s finest.  Supply and demand people.\",\"distinguished\":null,\"retrieved_on\":1473797860,\"gilded\":0,\"parent_id\":\"t1_cftzx\",\"edited\":false,\"id\":\"cfuty\",\"author\":\"[deleted]\",\"author_flair_text\":null}', '{\"author_flair_text\":null,\"author\":\"[deleted]\",\"edited\":false,\"id\":\"cfuu5\",\"parent_id\":\"t1_cfu53\",\"gilded\":0,\"retrieved_on\":1473797860,\"distinguished\":null,\"body\":\"\\\\\"As for sensing pain, that has absolutely nothing to do with murder.\\\\\"\\\\r\\\\n\\\\r\\\\nFor me, I feel that if a being has the ability to feel pain (whether it feels pain at death or not), then killing it is murder. \\\\r\\\\n\\\\r\\\\nSo yes, I murder cows because I eat their meat. I\\'m going to hell, and I don\\'t care!! But, hey, on the bright side, I can confess my sin and be forgiven :P\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_frco\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157070537,\"author_flair_css_class\":null,\"score\":0,\"ups\":0}', '{\"body\":\"lol\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fuu7\",\"stickied\":false,\"subreddit\":\"reddit.com\",\"score\":-1,\"ups\":-1,\"author_flair_css_class\":null,\"created_utc\":1157070665,\"author\":\"greenspans\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfuuc\",\"parent_id\":\"t3_fuu7\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860}', '{\"distinguished\":null,\"retrieved_on\":1473797862,\"gilded\":0,\"parent_id\":\"t3_frco\",\"edited\":false,\"id\":\"cfuuf\",\"author\":\"Smiley\",\"author_flair_text\":null,\"ups\":2,\"score\":2,\"author_flair_css_class\":null,\"created_utc\":1157070722,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_frco\",\"stickied\":false,\"body\":\"\\\\r\\\\nTo have sex with someone by force, against their will, without their consent is rape. The reason it is illegal is because in most civilized cultures humans are granted the right to bodily integrity, to be able to decide who they share their genitalia with.\\\\r\\\\n\\\\r\\\\nTo enslave someone is to have control over someone else\\'s life, they are not allowed to make basic decisions for themselves.\\\\r\\\\n\\\\r\\\\nTo remove the choice from a women regarding her own internal organs is to reduce her to a slave.\\\\r\\\\n\\\\r\\\\nYou can argue that life begins at conception all you want, but it does not change the fact that if the woman is forced by law to carry an unwanted pregnancy to term, that is a form of mental rape and physical slavery.\\\\r\\\\n\\\\r\\\\nTo argue anything else is illogical.   \\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\",\"controversiality\":0}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797862,\"author\":\"skykam\",\"author_flair_text\":null,\"parent_id\":\"t1_cfsss\",\"edited\":false,\"id\":\"cfuui\",\"subreddit\":\"reddit.com\",\"score\":16,\"ups\":16,\"author_flair_css_class\":null,\"created_utc\":1157070739,\"body\":\"&gt; but I can\\'t figure it out\\\\r\\\\n\\\\r\\\\nIn your last choice, instead of click, do a shift click.  You can then compare the new cards to what was on screen.  You can see that he has in fact removed all the cards that were there.\\\\r\\\\n\\\\r\\\\nI remember seeing this like 6-7 years ago.  Looks like it\\'s still going around the net.\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fsbz\",\"stickied\":false}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797862,\"author\":\"skykam\",\"author_flair_text\":null,\"id\":\"cfuul\",\"edited\":false,\"parent_id\":\"t1_cfsl8\",\"subreddit\":\"reddit.com\",\"score\":19,\"ups\":19,\"created_utc\":1157070771,\"author_flair_css_class\":null,\"controversiality\":0,\"body\":\"You have talent for management.  You\\'re hired.\\\\r\\\\n\\\\r\\\\n:-)\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fsbz\"}', '{\"distinguished\":null,\"retrieved_on\":1473797862,\"gilded\":0,\"edited\":false,\"id\":\"cfuus\",\"parent_id\":\"t3_fsbz\",\"author\":\"waxbolt\",\"author_flair_text\":null,\"ups\":4,\"score\":4,\"author_flair_css_class\":null,\"created_utc\":1157070881,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fsbz\",\"stickied\":false,\"controversiality\":0,\"body\":\"&gt; From: william C.  Sir, I took your experiment twice with the same positive results. Just yesterday I saw the same experiment overlaid over a \\\\\"David Copperfield\\\\\" presentation. I have no explaination other than perhaps a belief in experience-based reality. After participating in your experiment I decided that you picked the proper card because I wanted you to in the first place, and that I created the conditions to make that outcome come true by affecting my own perception of reality. Otherwise, perhaps the cards chose me instead of me choosing them? Regards, Bill\\\\r\\\\n\\\\r\\\\nSomehow, exactly correct.\\\\r\\\\n\\\\r\\\\nSomehow, exactly wrong?\"}', '{\"author_flair_text\":null,\"author\":\"Oak\",\"edited\":false,\"id\":\"cfuuy\",\"parent_id\":\"t1_cfsyk\",\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null,\"body\":\"*Yes, it is written in the Stars and it is your destiny...*\",\"controversiality\":0,\"link_id\":\"t3_fjeg\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157070964,\"score\":1,\"ups\":1}', '{\"edited\":false,\"parent_id\":\"t1_cfu6x\",\"id\":\"cfuv0\",\"author_flair_text\":null,\"author\":\"stomicron\",\"distinguished\":null,\"retrieved_on\":1473797862,\"gilded\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_frco\",\"stickied\":false,\"controversiality\":0,\"body\":\"I think you made some excellent points as well, but I just wanted to clarify. By *war* in my original comment I was referring to Iraq. Mine was a comment on what the political machine here has done to public opinion: firmly instilled the notion that supporting America necessitates supporting *this* war and in *this* way and that there can be no other way, which is simply a fallacy concocted by an administration that got entrenched in a war for one reason and then had to create alternative reasons (or excuses, be they correct or incorrect) when the first vanished.\\\\r\\\\n\\\\r\\\\n\\\\\"He who is not with me is against me\\\\\" doesn\\'t translate to \\\\\"He who disagrees with my action must disagree with my cause.\\\\\"\\\\r\\\\n\\\\r\\\\n\",\"score\":2,\"ups\":2,\"author_flair_css_class\":null,\"created_utc\":1157070965,\"subreddit\":\"reddit.com\"}', '{\"distinguished\":null,\"retrieved_on\":1473797862,\"gilded\":0,\"edited\":false,\"id\":\"cfuv2\",\"parent_id\":\"t1_cfsss\",\"author\":\"[deleted]\",\"author_flair_text\":null,\"score\":11,\"ups\":11,\"author_flair_css_class\":null,\"created_utc\":1157070972,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fsbz\",\"stickied\":false,\"controversiality\":0,\"body\":\"[deleted]\"}', '{\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157070988,\"score\":1,\"ups\":1,\"body\":\"like those losers at Google?\",\"controversiality\":0,\"link_id\":\"t3_fr4w\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"[deleted]\",\"edited\":false,\"id\":\"cfuv5\",\"parent_id\":\"t1_cfsfe\"}', '{\"link_id\":\"t3_futx\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"Only ~55 million kilometers separates them from each other.\",\"author_flair_css_class\":null,\"created_utc\":1157071059,\"score\":1,\"ups\":1,\"subreddit\":\"reddit.com\",\"parent_id\":\"t3_futx\",\"id\":\"cfuvg\",\"edited\":false,\"author_flair_text\":null,\"author\":\"digital\",\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797862,\"author\":\"jubjub123\",\"author_flair_text\":null,\"id\":\"cfuvp\",\"edited\":false,\"parent_id\":\"t3_ftam\",\"subreddit\":\"reddit.com\",\"score\":5,\"ups\":5,\"author_flair_css_class\":null,\"created_utc\":1157071145,\"controversiality\":0,\"body\":\"Don\\'t forget Auburn Calloway, who tried to [hijack]( http://en.wikipedia.org/wiki/FedEx_Flight_705) a DC-10 in 1994 and crash it into the FedEx Memphis headquarters.\\\\r\\\\n\\\\r\\\\nFunny how this was never publicized and everyone assumes that September 11th was the first use of airplane as a flying bomb.\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_ftam\",\"stickied\":false}', '{\"score\":-2,\"ups\":-2,\"created_utc\":1157071192,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_ftqa\",\"controversiality\":1,\"body\":\"Not a free lunch - \\\\r\\\\n\\\\r\\\\nhttp://news.bbc.co.uk/1/hi/england/beds/bucks/herts/5295244.stm\\\\r\\\\nhttp://news.bbc.co.uk/1/hi/magazine/4630938.stm \\\\r\\\\nhttp://news.bbc.co.uk/1/hi/health/4305783.stm \\\\r\\\\nhttp://news.bbc.co.uk/1/hi/health/4052963.stm \\\\r\\\\nhttp://news.bbc.co.uk/1/hi/uk/4013895.stm \\\\r\\\\nhttp://news.bbc.co.uk/1/hi/health/2407027.stm \\\\r\\\\nhttp://news.bbc.co.uk/1/hi/health/1146752.stm\\\\r\\\\n\\\\r\\\\nThese aren\\'t just some kind of made-up propaganda - I\\'ve seen this kind of thing happen and spoken with psychiatrists and nurses who have told me they see it all the time.\\\\r\\\\n\",\"distinguished\":null,\"retrieved_on\":1473797862,\"gilded\":0,\"id\":\"cfuvy\",\"parent_id\":\"t3_ftqa\",\"edited\":false,\"author\":\"chu\",\"author_flair_text\":null}', '{\"parent_id\":\"t3_ftqh\",\"id\":\"cfuw0\",\"edited\":false,\"author_flair_text\":null,\"author\":\"se7en\",\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"link_id\":\"t3_ftqh\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"Herman Goering (Nazi SA commander):\\\\r\\\\n\\\\r\\\\n\\\\\"Naturally the common people don\\'t want war; neither in Russia, nor in England, nor in America, nor in Germany. That is understood. But after all, it is the leaders of the country who determine policy, and it is always a simple matter to drag the people along, whether it is a democracy, or a fascist dictatorship, or a parliament, or a communist dictatorship. Voice or no voice, the people can always be brought to the bidding of the leaders. That is easy. All you have to do is to tell them they are being attacked, and denounce the pacifists for lack of patriotism and exposing the country to danger. It works the same in any country.\\\\\"\",\"author_flair_css_class\":null,\"created_utc\":1157071197,\"ups\":70,\"score\":70,\"subreddit\":\"reddit.com\"}', '{\"author_flair_text\":null,\"author\":\"digital\",\"id\":\"cfuw9\",\"edited\":false,\"parent_id\":\"t3_futr\",\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null,\"body\":\"Where\\'s Olive?\",\"controversiality\":0,\"link_id\":\"t3_futr\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157071310,\"score\":1,\"ups\":1}', '{\"created_utc\":1157071364,\"author_flair_css_class\":null,\"score\":-3,\"ups\":-3,\"subreddit\":\"reddit.com\",\"stickied\":false,\"link_id\":\"t3_ftqh\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"[deleted]\",\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"parent_id\":\"t3_ftqh\",\"edited\":false,\"id\":\"cfuwc\",\"author_flair_text\":null,\"author\":\"[deleted]\"}', '{\"author_flair_text\":null,\"author\":\"neilplatform1\",\"id\":\"cfuwg\",\"parent_id\":\"t3_fusp\",\"edited\":false,\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null,\"controversiality\":0,\"body\":\"it is 2\\\\r\\\\nblank verse\\\\r\\\\nunintelligible\\\\r\\\\nlike rosie\\\\r\\\\ncan\\'t be\\\\r\\\\nbothered\\\\r\\\\nstardom\",\"stickied\":false,\"link_id\":\"t3_fusp\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157071418,\"author_flair_css_class\":null,\"score\":1,\"ups\":1}', '{\"body\":\"Sure, that sounds like fun. Guess away. This could be nearly as entertaining as the ESP card trick comments.\",\"controversiality\":0,\"link_id\":\"t3_ft6a\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157071471,\"score\":1,\"ups\":1,\"author_flair_text\":null,\"author\":\"kanagawa\",\"id\":\"cfuwk\",\"edited\":false,\"parent_id\":\"t1_cftci\",\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797862,\"author\":\"mongonikol\",\"author_flair_text\":null,\"id\":\"cfuwn\",\"parent_id\":\"t3_fu5i\",\"edited\":false,\"subreddit\":\"reddit.com\",\"score\":0,\"ups\":0,\"created_utc\":1157071480,\"author_flair_css_class\":null,\"body\":\"Now *that* sounds like a big piece of Hollywood crapola.\\\\r\\\\n\\\\r\\\\nKing Kong stank. Who needs *another* war movie?\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fu5i\"}', '{\"body\":\"The Bush administration may not have lost any credibility with you but opinion polls show that it has lost credibility with the public in general.\",\"controversiality\":0,\"link_id\":\"t3_fpiu\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157071515,\"score\":1,\"ups\":1,\"author_flair_text\":null,\"author\":\"NewCrusader\",\"edited\":false,\"id\":\"cfuwr\",\"parent_id\":\"t1_cfubp\",\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null}', '{\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"[deleted]\",\"edited\":false,\"parent_id\":\"t3_fpaz\",\"id\":\"cfuwu\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157071528,\"score\":1,\"ups\":1,\"body\":\"Wonder how much we paid for that airport.\",\"controversiality\":0,\"link_id\":\"t3_fpaz\",\"stickied\":false,\"subreddit_id\":\"t5_6\"}', '{\"parent_id\":\"t3_fpaz\",\"edited\":false,\"id\":\"cfux0\",\"author_flair_text\":null,\"author\":\"[deleted]\",\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"stickied\":false,\"link_id\":\"t3_fpaz\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"Couldn\\'t we just build a [series of tubes](http://www.boldheaded.com/podcast/steves_viral/DJ_teds_techno_tubes.mp3)?\",\"created_utc\":1157071568,\"author_flair_css_class\":null,\"score\":2,\"ups\":2,\"subreddit\":\"reddit.com\"}', '{\"id\":\"cfux1\",\"parent_id\":\"t3_fumz\",\"edited\":false,\"author_flair_text\":null,\"author\":\"ellsmall\",\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"link_id\":\"t3_fumz\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"CNN are traitors, they revealed our classified warp drive technology, they might as well go ahead and publish the location of our clone army breeding facility.\",\"author_flair_css_class\":null,\"created_utc\":1157071610,\"score\":12,\"ups\":12,\"subreddit\":\"reddit.com\"}', '{\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"id\":\"cfux2\",\"edited\":false,\"parent_id\":\"t1_cfujr\",\"author_flair_text\":null,\"author\":\"Oak\",\"created_utc\":1157071659,\"author_flair_css_class\":null,\"score\":3,\"ups\":3,\"subreddit\":\"reddit.com\",\"stickied\":false,\"link_id\":\"t3_fr8a\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"No secret here.\\\\r\\\\n\\\\r\\\\nAmericans hate who they\\'re told to hate.\"}', '{\"parent_id\":\"t1_cfujr\",\"edited\":false,\"id\":\"cfux8\",\"author\":\"[deleted]\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fr8a\",\"controversiality\":0,\"body\":\"[deleted]\",\"score\":1,\"ups\":1,\"created_utc\":1157071738,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\"}', '{\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0,\"id\":\"cfuxg\",\"edited\":false,\"parent_id\":\"t1_cfusn\",\"author\":\"nostrademons\",\"author_flair_text\":null,\"score\":-1,\"ups\":-1,\"author_flair_css_class\":null,\"created_utc\":1157071782,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_frco\",\"stickied\":false,\"controversiality\":1,\"body\":\"I was going for the Russell\\'s Paradox effect.\\\\r\\\\n\\\\r\\\\nAnd your sentence isn\\'t really a Russell\\'s Paradox problem, because the speaker is unambiguously in the one kind of people: those who are referenced by this very sentence.  It is, however, amusing in the \\\\\"There are three kinds of people in this world: those who can count, and those who can\\'t\\\\\" sense.  When I constructed the sentence in the grandparent, I had to make sure that the speaker would be referenced by the last clause iff he was not referenced by the last clause.  So if he\\'s in the first category (those referenced by the last clause), he cannot be in the second by the Law of Excluded Middle (assumed in all these \\\\\"There are two kinds of people\\\\\" jokes...), yet he must be because of the statement\\'s content.\"}', '{\"author\":\"[deleted]\",\"author_flair_text\":null,\"id\":\"cfuxo\",\"edited\":false,\"parent_id\":\"t3_fsbz\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797863,\"body\":\"Before you post the real explanation, have the humility to assume that other redditors are as smart as you... because they\\'re probably smarter.\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fsbz\",\"subreddit\":\"reddit.com\",\"score\":2,\"ups\":2,\"created_utc\":1157071856,\"author_flair_css_class\":null}', '{\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fsjp\",\"stickied\":false,\"controversiality\":1,\"body\":\"Actually, my sister generally hates women-only groups, because women apparently become catty bitches when no guys are around.  But she also hates being harassed and leered at.  So yes, given those choices, I bet she\\'d go for the women-only car - but she shouldn\\'t have to make that choice!  Why can\\'t we have unsegregated cars where people respect each other?\",\"score\":0,\"ups\":0,\"author_flair_css_class\":null,\"created_utc\":1157071918,\"subreddit\":\"reddit.com\",\"id\":\"cfuxv\",\"edited\":false,\"parent_id\":\"t1_cfudx\",\"author\":\"nostrademons\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0}', '{\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0,\"parent_id\":\"t1_cfult\",\"id\":\"cfuya\",\"edited\":false,\"author\":\"sorbix\",\"author_flair_text\":null,\"score\":14,\"ups\":14,\"author_flair_css_class\":null,\"created_utc\":1157072134,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_ftqa\",\"stickied\":false,\"controversiality\":0,\"body\":\"I don\\'t agree.  Maybe it\\'s just the fact that I live in California, but almost all of my friends back in high school were much more concerned with their parents knowing they smoke tobacco than pot.  Cigarettes are definitely worse for you... by a long shot too.\"}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797863,\"author_flair_text\":null,\"author\":\"NewCrusader\",\"edited\":false,\"parent_id\":\"t3_fulh\",\"id\":\"cfuyb\",\"subreddit\":\"reddit.com\",\"ups\":1,\"score\":1,\"created_utc\":1157072135,\"author_flair_css_class\":null,\"controversiality\":0,\"body\":\"They produce a show that is hip, smart, funny, irreverant, and intelligent.  What sort of views might the creators of *South Park* hold?\\\\r\\\\n\\\\r\\\\n&gt;Stone and Parker said that if you had to put a label on them, they were **libertarian** \\\\u2014 and that didn\\'t mean Republican to this crowd.\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fulh\"}', '{\"edited\":false,\"parent_id\":\"t3_ful7\",\"id\":\"cfuyh\",\"author\":\"NewCrusader\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_ful7\",\"stickied\":false,\"body\":\"Is it \\\\\"bashing\\\\\" when it\\'s factual?\",\"controversiality\":0,\"score\":0,\"ups\":0,\"author_flair_css_class\":null,\"created_utc\":1157072237,\"subreddit\":\"reddit.com\"}', '{\"edited\":false,\"id\":\"cfuyk\",\"parent_id\":\"t1_cfubd\",\"author_flair_text\":null,\"author\":\"nostrademons\",\"retrieved_on\":1473797863,\"distinguished\":null,\"gilded\":0,\"link_id\":\"t3_ftqh\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"More to the point, they were levied with war debts they simply couldn\\'t pay, and so tried to inflate the currency to raise funds, which led to massive hyperinflation and widespread economic instability.\\\\r\\\\n\\\\r\\\\nThis makes me really worried about *our* war debts (which thankfully aren\\'t a huge portion of the GDP *yet*, but are rapidly going in the wrong direction).  U.S. debt is denominated in dollars, which makes an inflation of the currency even more likely (unlike Germany, we *could* wipe out our war debts by printing money).  That\\'s just as bad for American citizens though, and will win us no friends abroad.\\\\r\\\\n\\\\r\\\\nI\\'m actually far more worried about U.S. inflation due to terrorism than I am about terrorism itself.  I suspect Osama bin Laden\\'s purpose in blowing up the twin towers wasn\\'t to tear America apart, but to make us so paranoid that we tear ourselves apart.  And he\\'s been doing a fairly good job of it so far.\",\"author_flair_css_class\":null,\"created_utc\":1157072290,\"score\":22,\"ups\":22,\"subreddit\":\"reddit.com\"}', '{\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0,\"id\":\"cfuyn\",\"parent_id\":\"t3_ftam\",\"edited\":false,\"author\":\"[deleted]\",\"author_flair_text\":null,\"score\":-6,\"ups\":-6,\"created_utc\":1157072331,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_ftam\",\"controversiality\":1,\"body\":\"this is exactly why the term islamic fascism is so accurate, clear and useful. islam is not the problem. terrorism is not the problem. islamic fascism is the problem. terrorism is a tactic, islamic fascists, not terrorists, are the enemy.\\\\r\\\\n\\\\r\\\\nthe focus on terrorism is a misleading distraction.\"}', '{\"link_id\":\"t3_ftrs\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"I really hope Pluto stays a dwarf planet because one, I never have really considered Pluto a planet and two, they are going to have to add a bunch of other \\\\\"planets\\\\\" with Pluto. Planets are supposed to be special, dammit!\",\"author_flair_css_class\":null,\"created_utc\":1157072416,\"score\":1,\"ups\":1,\"subreddit\":\"reddit.com\",\"id\":\"cfuyr\",\"edited\":false,\"parent_id\":\"t3_ftrs\",\"author\":\"vipstar\",\"author_flair_text\":null,\"retrieved_on\":1473797863,\"distinguished\":null,\"gilded\":0}', '{\"gilded\":0,\"retrieved_on\":1473797863,\"distinguished\":null,\"author\":\"markedtrees\",\"author_flair_text\":null,\"parent_id\":\"t3_foll\",\"id\":\"cfuyv\",\"edited\":false,\"subreddit\":\"programming\",\"author_flair_css_class\":null,\"created_utc\":1157072514,\"score\":1,\"ups\":1,\"body\":\"[Dupe](http://programming.reddit.com/info/foh3/comments). Or so I think. Comparing \\\\\"1 day ago\\\\\" and \\\\\"1 day ago\\\\\" isn\\'t the easiest thing to do.\",\"controversiality\":0,\"link_id\":\"t3_foll\",\"stickied\":false,\"subreddit_id\":\"t5_2fwo\"}', '{\"subreddit\":\"reddit.com\",\"score\":7,\"ups\":7,\"author_flair_css_class\":null,\"created_utc\":1157072647,\"controversiality\":0,\"body\":\"Two of those are antecdotal, which is not scientific.\\\\r\\\\n\\\\r\\\\nOne of them is just telling us about hte rising statistics of drug use... not an inherently bad thing.\\\\r\\\\n\\\\r\\\\nAnd the rest of them describe pot as causing mental health problems, which are very hard to pin down.  Psychology is not a strict science and mis-diagnosis is extremely common.\\\\r\\\\n\\\\r\\\\nOn the other hand, the positive effects of marijuana are more clearly documented and are MUCH more scientifically observable.\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_ftqa\",\"stickied\":false,\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797863,\"author_flair_text\":null,\"author\":\"sorbix\",\"id\":\"cfuz5\",\"parent_id\":\"t1_cfuvy\",\"edited\":false}', '{\"score\":1,\"ups\":1,\"created_utc\":1157072835,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_ftam\",\"body\":\"right, because all might-be-a-terrorist folks are going from the same place to the same place.\",\"controversiality\":0,\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0,\"parent_id\":\"t1_cfuis\",\"edited\":false,\"id\":\"cfuzj\",\"author_flair_text\":null,\"author\":\"sblinn\"}', '{\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fsjp\",\"stickied\":false,\"controversiality\":0,\"body\":\"That\\'s not buying time, it\\'s buying convenience. If you get held up on the way to work, that\\'s not money lost, and it\\'s sad to think so (probably causes road rage too). Similarly, the dude in the link is arguing about the convenience, which might not be worth as much to someone as the benefits of using public transit.\",\"score\":-9,\"ups\":-9,\"author_flair_css_class\":null,\"created_utc\":1157072840,\"subreddit\":\"reddit.com\",\"parent_id\":\"t1_cfuaz\",\"edited\":false,\"id\":\"cfuzk\",\"author\":\"[deleted]\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797863,\"gilded\":0}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797863,\"author\":\"sblinn\",\"author_flair_text\":null,\"id\":\"cfuzn\",\"edited\":false,\"parent_id\":\"t3_ftam\",\"subreddit\":\"reddit.com\",\"score\":-5,\"ups\":-5,\"created_utc\":1157072897,\"author_flair_css_class\":null,\"body\":\"Flying on an aeroplane is a privilege, not a right.\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_ftam\"}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797863,\"author\":\"animeh\",\"author_flair_text\":null,\"id\":\"cfuzt\",\"parent_id\":\"t3_fuzq\",\"edited\":false,\"subreddit\":\"reddit.com\",\"score\":1,\"ups\":1,\"created_utc\":1157072965,\"author_flair_css_class\":null,\"body\":\"the best\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fuzq\"}', '{\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fuzw\",\"stickied\":false,\"controversiality\":0,\"body\":\"Over 3 Million Listings, Instant Home Valuations, Interactive Maps and More.\",\"score\":1,\"ups\":1,\"author_flair_css_class\":null,\"created_utc\":1157073028,\"subreddit\":\"reddit.com\",\"id\":\"cfuzy\",\"edited\":false,\"parent_id\":\"t3_fuzw\",\"author\":\"pnk_master\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797864,\"gilded\":0}', '{\"body\":\"I think I said this already here at reddit, but anyway . . . \\\\r\\\\n\\\\r\\\\nI used to support Isreal.  That was before I found my brain.  They obviously want to be a mini US, always pushing other countries around like a big bully.  \\\\r\\\\n\\\\r\\\\nIt\\'s ok when we do it, but other countries aren\\'t allowed to do the same.  Hypocrites, all.\",\"controversiality\":0,\"link_id\":\"t3_fqri\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157073076,\"score\":3,\"ups\":3,\"author_flair_text\":null,\"author\":\"Smiley\",\"parent_id\":\"t3_fqri\",\"edited\":false,\"id\":\"cfv02\",\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null}', '{\"score\":7,\"ups\":7,\"author_flair_css_class\":null,\"created_utc\":1157073106,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fumz\",\"stickied\":false,\"controversiality\":0,\"body\":\"If it was really travelling that fast, the crew could just go back in time and fix the mistake.\",\"distinguished\":null,\"retrieved_on\":1473797864,\"gilded\":0,\"id\":\"cfv07\",\"parent_id\":\"t3_fumz\",\"edited\":false,\"author\":\"clinintern\",\"author_flair_text\":null}', '{\"stickied\":false,\"link_id\":\"t3_foz0\",\"subreddit_id\":\"t5_2fwo\",\"controversiality\":0,\"body\":\"I disagree, but I am biased. I think OO in JavaScript should be kept hard because it\\'s inherently a prototyping language. I really don\\'t want to see JS become an OOP + prototyping language, and I really don\\'t want to see JS become an OOP language. I like the prototyping paradigm; I enjoy the simplicity of not being exposed to complex inheritance or access rules, and I think it\\'s suited to web development [1]. I enjoy JS syntax; I don\\'t want to see that clouded with access or inheritance modifiers. I think certain other paradigms are better suited to prototyping; for example, aspects-oriented programming is a lot more function-oriented and could even simplify OOP implementations in JS [2].\\\\r\\\\n\\\\r\\\\n[1] Lua has proven that it works for scripting as well.\\\\r\\\\n\\\\r\\\\n[2] I envision a future where functions (which are objects) are pointcuts and join points are `Function.prototype` methods and you can do neat things like `alert.after += function {...}`.\",\"created_utc\":1157073118,\"author_flair_css_class\":null,\"score\":1,\"ups\":1,\"subreddit\":\"programming\",\"parent_id\":\"t1_cfphq\",\"edited\":false,\"id\":\"cfv09\",\"author_flair_text\":null,\"author\":\"markedtrees\",\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0}', '{\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157073247,\"score\":2,\"ups\":2,\"body\":\"Sure, blame the arrow, not the archer....\",\"controversiality\":0,\"link_id\":\"t3_fs2n\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"[deleted]\",\"parent_id\":\"t3_fs2n\",\"id\":\"cfv0e\",\"edited\":false}', '{\"body\":\"Even funnier, the Wikipedia article refers to the intended hijacking as a kamikaze attempt.  I don\\'t believe I have ever heard the 9/11 attacks referred to as such.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_ftam\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157073453,\"author_flair_css_class\":null,\"score\":1,\"ups\":1,\"author_flair_text\":null,\"author\":\"[deleted]\",\"parent_id\":\"t1_cfuvp\",\"id\":\"cfv0p\",\"edited\":false,\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null}', '{\"author\":\"markedtrees\",\"author_flair_text\":null,\"id\":\"cfv0y\",\"parent_id\":\"t1_cfs6s\",\"edited\":false,\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797864,\"body\":\"None of this is a new language feature. This is a technique building upon delegates using the fact that you can create an array of delegates (or any type) in C#. This is no different than creating an array of (first-class) functions and iterating over them, calling each one in a more dynamic language: You\\'re not using any new feature; you\\'re simply using old features better [1].\\\\r\\\\n\\\\r\\\\n[1] Here, that\\'s not really true since C# has extremely wordy syntax for delegates, and delegates are a bad replacement for first-class functions.\",\"controversiality\":0,\"subreddit_id\":\"t5_2fwo\",\"link_id\":\"t3_fqr5\",\"stickied\":false,\"subreddit\":\"programming\",\"score\":1,\"ups\":1,\"author_flair_css_class\":null,\"created_utc\":1157073644}', '{\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"smoovement\",\"parent_id\":\"t1_cfs1c\",\"edited\":false,\"id\":\"cfv0z\",\"subreddit\":\"reddit.com\",\"created_utc\":1157073662,\"author_flair_css_class\":null,\"score\":-2,\"ups\":-2,\"body\":\"Well the problem is that no one condemns him.  What\\'s even more funny about this whole situation is to think that David Duke has to travel to Syria to find a welcome TV audience.\\\\r\\\\n\\\\r\\\\nhttp://www.memritv.org/search.asp?ACT=S9&amp;P1=941\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_fo43\",\"subreddit_id\":\"t5_6\"}', '{\"author_flair_text\":null,\"author\":\"smoovement\",\"edited\":false,\"parent_id\":\"t1_cft7z\",\"id\":\"cfv14\",\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"body\":\"http://www.memritv.org/search.asp?ACT=S9&amp;P1=941\\\\r\\\\n\\\\r\\\\nHey, if we can send anyone over let\\'s send David Duke back to Syria where he can get another warm reception on state TV!\",\"controversiality\":0,\"link_id\":\"t3_fo43\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157073707,\"score\":-3,\"ups\":-3}', '{\"subreddit_id\":\"t5_2fwo\",\"stickied\":false,\"link_id\":\"t3_fr2p\",\"controversiality\":0,\"body\":\"There were also some amusing error messages in old Lisp systems. Like these from MacLisp due to Guy Steele:\\\\r\\\\n\\\\r\\\\n    VERITAS AETERNA--don\\'t SETQ T\\\\r\\\\n    NIHIL EX NIHIL--don\\'t SETQ NIL\",\"ups\":3,\"score\":3,\"created_utc\":1157073757,\"author_flair_css_class\":null,\"subreddit\":\"programming\",\"edited\":false,\"id\":\"cfv17\",\"parent_id\":\"t3_fr2p\",\"author\":\"psykotic\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797864,\"gilded\":0}', '{\"subreddit\":\"reddit.com\",\"score\":-1,\"ups\":-1,\"created_utc\":1157073819,\"author_flair_css_class\":null,\"controversiality\":0,\"body\":\"Ohhhhhhhhh!  He apologized?  Do you mean the way Trent Lott apologized for his commment?  Something that \\\\\"hinted\\\\\" of racism rather than being associated for years with an organization formed on the basis of racism?\\\\r\\\\n\\\\r\\\\nTell you what.  If you are so interested in the KKK, take a guess at where they have to travel to get their message out.\\\\r\\\\n\\\\r\\\\nhttp://www.memritv.org/search.asp?ACT=S9&amp;P1=941\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fo43\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797864,\"author\":\"smoovement\",\"author_flair_text\":null,\"edited\":false,\"parent_id\":\"t1_cfsb9\",\"id\":\"cfv19\"}', '{\"author\":\"[deleted]\",\"author_flair_text\":null,\"id\":\"cfv1a\",\"edited\":false,\"parent_id\":\"t1_cfu68\",\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"body\":\"[deleted]\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_frl8\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157073837,\"author_flair_css_class\":null,\"score\":1,\"ups\":1}', '{\"id\":\"cfv1c\",\"parent_id\":\"t1_cftk7\",\"edited\":false,\"author\":\"[deleted]\",\"author_flair_text\":null,\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0,\"stickied\":false,\"link_id\":\"t3_fsjp\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"[deleted]\",\"created_utc\":1157073851,\"author_flair_css_class\":null,\"score\":-3,\"ups\":-3,\"subreddit\":\"reddit.com\"}', '{\"stickied\":false,\"link_id\":\"t3_fo43\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"I certainly do, though he\\'s come a long way toward repentance, I think. What ever does David Duke have to do with anything? \\\\r\\\\n\\\\r\\\\nNever mind. I don\\'t think I care to follow the non sequiturs any longer.\",\"created_utc\":1157073857,\"author_flair_css_class\":null,\"ups\":2,\"score\":2,\"subreddit\":\"reddit.com\",\"edited\":false,\"id\":\"cfv1d\",\"parent_id\":\"t1_cfv0z\",\"author_flair_text\":null,\"author\":\"cleanthes\",\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0}', '{\"subreddit\":\"reddit.com\",\"ups\":1,\"score\":1,\"created_utc\":1157073889,\"author_flair_css_class\":null,\"body\":\"[deleted]\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_frl8\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797864,\"author\":\"[deleted]\",\"author_flair_text\":null,\"edited\":false,\"parent_id\":\"t1_cfu68\",\"id\":\"cfv1h\"}', '{\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_frl8\",\"controversiality\":0,\"body\":\"Sure.  It\\'s a matter of intent.  If there\\'s a God who wills all things according to His purpose, He can just as easily have people always regrow their limbs, right?  But that\\'s clearly not how things work in this world we\\'re in.  And yet people still are missing limbs in this world where they don\\'t grow back.  God actually intends these things to happen.\\\\r\\\\n\\\\r\\\\nWhy?  Forget for a second why God doesn\\'t heal amputees, a far tougher question to face is this -- \\\\\"why doesn\\'t God heal _me_?\\\\\"  Even Paul, an apostle from the Bible, tells that he prayed repeatedly for God to remove him from some agonizing situation... and God didn\\'t.  What\\'s the deal?\\\\r\\\\n\\\\r\\\\nThere are a lot of complicated explanations, but for Christians it all comes down to this.  God says to all people, \\\\\"I love you.  Trust in Me.\\\\\"  You either you believe this or you don\\'t.  If you do, no explanation is necessary.  If you don\\'t, no explanation is possible.\\\\r\\\\n\\\\r\\\\nHard to swallow, isn\\'t it?\",\"score\":2,\"ups\":2,\"created_utc\":1157073951,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"edited\":false,\"id\":\"cfv1l\",\"parent_id\":\"t1_cfu68\",\"author\":\"jchaps\",\"author_flair_text\":null,\"distinguished\":null,\"retrieved_on\":1473797864,\"gilded\":0}', '{\"created_utc\":1157073987,\"author_flair_css_class\":null,\"score\":-1,\"ups\":-1,\"subreddit\":\"reddit.com\",\"stickied\":false,\"link_id\":\"t3_fj97\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"Hey, where can I get a good deal on a vest and ski mask?\",\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0,\"parent_id\":\"t1_cfnmp\",\"edited\":false,\"id\":\"cfv1p\",\"author_flair_text\":null,\"author\":\"smoovement\"}', '{\"stickied\":false,\"link_id\":\"t3_fsgf\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"Your point?\",\"created_utc\":1157073987,\"author_flair_css_class\":null,\"score\":4,\"ups\":4,\"subreddit\":\"reddit.com\",\"edited\":false,\"id\":\"cfv1r\",\"parent_id\":\"t1_cfuty\",\"author\":\"ab3nnion\",\"author_flair_text\":null,\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0}', '{\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797864,\"author_flair_text\":null,\"author\":\"markedtrees\",\"id\":\"cfv1u\",\"parent_id\":\"t3_fr8c\",\"edited\":false,\"subreddit\":\"programming\",\"score\":1,\"ups\":1,\"created_utc\":1157074028,\"author_flair_css_class\":null,\"controversiality\":0,\"body\":\"&gt; C# seems to have hundreds of different collection classes, used inconsistently in the .NET libraries.\\\\r\\\\n\\\\r\\\\nAnd they\\'re all under `System.Collections` [and are oh-so hard to count. They are: ArrayList, BitArray, Hashtable, Queue, SortedList, and Stack. (Hundreds, I know [1].)](http://msdn2.microsoft.com/en-us/library/system.collections.aspx) And they all implement interfaces defined in .NET (IComparable, IEnumerator, IDictionary, IWeLoveInterfacesHowAboutYou), ensuring consistency between collection types and mentioned countless times in documentationz not to mention that the collection types themselves are extremely well-documented. Oh yeah, it\\'s a real mess.\\\\r\\\\n\\\\r\\\\n[1] The rest are abstract classes or those weird case comparer classes that nobody uses except the people who write .NET.\",\"subreddit_id\":\"t5_2fwo\",\"stickied\":false,\"link_id\":\"t3_fr8c\"}', '{\"edited\":false,\"parent_id\":\"t3_ftfj\",\"id\":\"cfv1v\",\"author_flair_text\":null,\"author\":\"schwarzwald\",\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0,\"stickied\":false,\"link_id\":\"t3_ftfj\",\"subreddit_id\":\"t5_2fwo\",\"body\":\"Everyone drools over Google, but notice how they can only work on stuff that scales to millions of people. You can\\'t work on niche products.\",\"controversiality\":0,\"created_utc\":1157074090,\"author_flair_css_class\":null,\"score\":-5,\"ups\":-5,\"subreddit\":\"programming\"}', '{\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157074159,\"score\":1,\"ups\":1,\"body\":\"[deleted]\",\"controversiality\":0,\"link_id\":\"t3_ftam\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"[deleted]\",\"id\":\"cfv1y\",\"parent_id\":\"t1_cftzd\",\"edited\":false}', '{\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"TronXD\",\"id\":\"cfv20\",\"edited\":false,\"parent_id\":\"t1_cfup2\",\"subreddit\":\"reddit.com\",\"created_utc\":1157074173,\"author_flair_css_class\":null,\"score\":2,\"ups\":2,\"body\":\"&gt;Did they write it in Ruby?\\\\r\\\\n\\\\r\\\\nThat is the worst attempt to jab at a language I\\'ve ever seen.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_ft6a\",\"subreddit_id\":\"t5_6\"}', '{\"id\":\"cfv27\",\"parent_id\":\"t1_cftwl\",\"edited\":false,\"author\":\"stomicron\",\"author_flair_text\":null,\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0,\"stickied\":false,\"link_id\":\"t3_ftam\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"&gt; you assume the worst (white,black, boogeyman whatever ) - you profile and take action.\\\\r\\\\n\\\\r\\\\nAh...but that\\'s not profiling then is it? Assuming the worst regardless of what the person looks like is what we *should* be doing, as this author points out.\\\\r\\\\n\\\\r\\\\n\",\"created_utc\":1157074319,\"author_flair_css_class\":null,\"score\":5,\"ups\":5,\"subreddit\":\"reddit.com\"}', '{\"distinguished\":null,\"retrieved_on\":1473797864,\"gilded\":0,\"parent_id\":\"t1_cfu4o\",\"edited\":false,\"id\":\"cfv28\",\"author_flair_text\":null,\"author\":\"dsearson\",\"score\":1,\"ups\":1,\"author_flair_css_class\":null,\"created_utc\":1157074328,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fr8b\",\"stickied\":false,\"body\":\"You could probably start it up with a second small motor. Remember it runs at 102rpm so you only need to turn the motor over about twice per second to get it going. And you only need to overcome the engine\\'s internal resistance (from bearings, piston seals etc) in order for it to start.\",\"controversiality\":0}', '{\"author_flair_text\":null,\"author\":\"joshd\",\"id\":\"cfv29\",\"edited\":false,\"parent_id\":\"t1_cfrle\",\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"body\":\"You say that the climate model is \\\\\"tenuous at best\\\\\", yet your previous comments show you don\\'t understand it at all. In any case, I believe that there is a chance that the predictions could be wrong, however when you look at the potential consequences then you _have_ to take some action. \\\\r\\\\n\\\\r\\\\nThe state I live in is currently experiencing the worst drought in recorded history. It has cost billions in government subsidies and new infrustructure needed to handle  the situation. If the predictions about global warming are correct then this cost of this drought will seem insignificant. You are talking about droughts and floods wiping out farmland, leading to global famines. Storms and rising sea levels will mean that billions of people will be displaced. As advanced as you may think we are, we cannot handle the consequences. Look at how poorly the evecuation and cleanup in New Orleans went. Now imagine that taking place in hundreds of cities worldwide.\\\\r\\\\n\\\\r\\\\nAt the moment there is no insentive for those contributing to the problem to make a difference. People will consume whatever they can, and ignore the consequences. Corporations have shown time and time again that they are willing to put short term profits above everything else. Even _if_ we knew for sure that global warming was going to produce disasterous effects do you think that they would do the right thing?\\\\r\\\\n\\\\r\\\\nWorking towards fixing the problem may result to a hit to economic activity, but at least it will be controllable. \\\\r\\\\nMeasures like carbon trading schemes would provide econimic insentives for the companies to do the right thing. Its benifits are two-fold. It will create new industries and benifit companies who invest in new technology, and it can be scaled up over a period of time which means that we can combat the problem without placing immediate strain on existing companies.\\\\r\\\\n\\\\r\\\\nClean energy is going to take decades to produce and become dominate. The majority of the world\\'s electricity is produces by coal power plants, yet we have no way to retrofit older plants, or even build new ones to make them \\\\\"clean\\\\\". Such technology looks like it is at least 15 years off, even if we fully commit to it now. If we continue to do nothing then we run the risk of realising in 50 years that there is a massive problem, but we have no way of fighting it.\",\"controversiality\":0,\"link_id\":\"t3_fpmb\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157074347,\"score\":0,\"ups\":0}', '{\"body\":\"Olbermann just might be the Edward R. Murrow of our generation\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fv26\",\"stickied\":false,\"subreddit\":\"reddit.com\",\"score\":4,\"ups\":4,\"author_flair_css_class\":null,\"created_utc\":1157074358,\"author\":\"varkam\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfv2b\",\"parent_id\":\"t3_fv26\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797864}', '{\"score\":2,\"ups\":2,\"author_flair_css_class\":null,\"created_utc\":1157074371,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"link_id\":\"t3_fsjp\",\"stickied\":false,\"controversiality\":0,\"body\":\"[deleted]\",\"distinguished\":null,\"retrieved_on\":1473797864,\"gilded\":0,\"parent_id\":\"t1_cfuxv\",\"id\":\"cfv2c\",\"edited\":false,\"author\":\"[deleted]\",\"author_flair_text\":null}', '{\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"smoovement\",\"edited\":false,\"parent_id\":\"t1_cfk5i\",\"id\":\"cfv2d\",\"subreddit\":\"reddit.com\",\"created_utc\":1157074401,\"author_flair_css_class\":null,\"score\":1,\"ups\":1,\"body\":\"This is an example of how much you people understand history.  Do you have any idea of what happened after the US left Vietnam?  Do you care?  Probably not, communists killing 3 million people is nothing compared to what the good that they brought with them.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_fj97\",\"subreddit_id\":\"t5_6\"}', '{\"body\":\"I love how the author smoothly transitions from process priorities to thread priorities with nary a blink of an eye. This is only then explained in a comment (search for \\\\\"&gt; Don\\'t\\\\\" since his comment system is too good to have permalinks) after somebody brings it up.\",\"controversiality\":0,\"subreddit_id\":\"t5_2fwo\",\"stickied\":false,\"link_id\":\"t3_fth6\",\"subreddit\":\"programming\",\"score\":0,\"ups\":0,\"created_utc\":1157074429,\"author_flair_css_class\":null,\"author\":\"markedtrees\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfv2e\",\"parent_id\":\"t3_fth6\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797864}', '{\"created_utc\":1157074454,\"author_flair_css_class\":null,\"score\":0,\"ups\":0,\"subreddit\":\"reddit.com\",\"stickied\":false,\"link_id\":\"t3_cr5b\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"this is ancient i know...i am a human female, by the way, and i tend to over cut the hair to very short then let it grow for 10 months at a time with no apparent need for cutting until a certain compulsion kind of makes me do so.  strange and ritualistic behavior.\",\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0,\"edited\":false,\"id\":\"cfv2h\",\"parent_id\":\"t1_ccstn\",\"author_flair_text\":null,\"author\":\"liberatedword\"}', '{\"gilded\":0,\"retrieved_on\":1473797864,\"distinguished\":null,\"author_flair_text\":null,\"author\":\"[deleted]\",\"edited\":false,\"id\":\"cfv2i\",\"parent_id\":\"t1_cfuzj\",\"subreddit\":\"reddit.com\",\"created_utc\":1157074459,\"author_flair_css_class\":null,\"score\":-2,\"ups\":-2,\"body\":\"[deleted]\",\"controversiality\":1,\"stickied\":false,\"link_id\":\"t3_ftam\",\"subreddit_id\":\"t5_6\"}', '{\"id\":\"cfv2j\",\"parent_id\":\"t3_fsbz\",\"edited\":false,\"author_flair_text\":null,\"author\":\"[deleted]\",\"retrieved_on\":1473797864,\"distinguished\":null,\"gilded\":0,\"stickied\":false,\"link_id\":\"t3_fsbz\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"[deleted]\",\"created_utc\":1157074472,\"author_flair_css_class\":null,\"score\":1,\"ups\":1,\"subreddit\":\"reddit.com\"}', '{\"retrieved_on\":1473797866,\"distinguished\":null,\"gilded\":0,\"edited\":false,\"parent_id\":\"t1_cfu0e\",\"id\":\"cfv2m\",\"author_flair_text\":null,\"author\":\"stomicron\",\"author_flair_css_class\":null,\"created_utc\":1157074554,\"score\":-2,\"ups\":-2,\"subreddit\":\"reddit.com\",\"link_id\":\"t3_ftam\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"I downmodded you because\\\\r\\\\n\\\\r\\\\n&gt; I think you can make the case that [racial profiling] would be more effective than random searches\\\\r\\\\n\\\\r\\\\ncontradicts the point of this story.\"}', '{\"body\":\"Guess he didn\\'t have enough faith!\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fra8\",\"subreddit\":\"reddit.com\",\"score\":2,\"ups\":2,\"created_utc\":1157074585,\"author_flair_css_class\":null,\"author\":\"[deleted]\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfv2o\",\"parent_id\":\"t3_fra8\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797866}', '{\"stickied\":false,\"link_id\":\"t3_fsjp\",\"subreddit_id\":\"t5_6\",\"controversiality\":1,\"body\":\"[deleted]\",\"created_utc\":1157074637,\"author_flair_css_class\":null,\"score\":-1,\"ups\":-1,\"subreddit\":\"reddit.com\",\"edited\":false,\"id\":\"cfv2q\",\"parent_id\":\"t1_cftss\",\"author_flair_text\":null,\"author\":\"[deleted]\",\"retrieved_on\":1473797866,\"distinguished\":null,\"gilded\":0}', '{\"subreddit\":\"programming\",\"score\":1,\"ups\":1,\"created_utc\":1157074644,\"author_flair_css_class\":null,\"body\":\"&gt; Vim7.0 tabs suck\\\\r\\\\n\\\\r\\\\nFrom the rediquette:\\\\r\\\\n\\\\r\\\\n&gt; Editorialize in the headlines.\\\\r\\\\n\\\\r\\\\nFrom common sense:\\\\r\\\\n\\\\r\\\\n&gt; People have no idea what you\\'re talking about if you link to a vim tips page and go \\\\\"Vim 7.0 tabs suck, skeet skeet.\\\\\"\\\\r\\\\n\\\\r\\\\nFrom me:\\\\r\\\\n\\\\r\\\\n&gt; I think dean is actually referring to the comment made by nowayjose.\",\"controversiality\":0,\"subreddit_id\":\"t5_2fwo\",\"stickied\":false,\"link_id\":\"t3_fu6h\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797866,\"author\":\"markedtrees\",\"author_flair_text\":null,\"edited\":false,\"id\":\"cfv2r\",\"parent_id\":\"t3_fu6h\"}', '{\"parent_id\":\"t1_cfup8\",\"edited\":false,\"id\":\"cfv2u\",\"author\":\"nostrademons\",\"author_flair_text\":null,\"retrieved_on\":1473797866,\"distinguished\":null,\"gilded\":0,\"link_id\":\"t3_ftam\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":1,\"body\":\"Garlic, crosses, and fire?\",\"author_flair_css_class\":null,\"created_utc\":1157074677,\"score\":4,\"ups\":4,\"subreddit\":\"reddit.com\"}']\n"
     ]
    }
   ],
   "source": [
    "comments = rdd.take(1000)\n",
    "print comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments = [c.lower() for c in comments]\n",
    "keywords = ['Olive', 'Personally', 'Hiding', 'American']\n",
    "keywords = [w.lower() for w in words] \n",
    "\n",
    "matches = [c for c in comments if any(w in c for w in keywords)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ups\":3,\"score\":3,\"created_utc\":1157069796,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fq6q\",\"body\":\"personally, i wish that the population of every country, not just america, was exposed to critical and potent journalism all the time. \\r\\n\\r\\nit doesn't have to be right. it just has to show that thinking is allowed, dissent is encouraged, and free speech means something.\",\"controversiality\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"gilded\":0,\"id\":\"cfusq\",\"edited\":false,\"parent_id\":\"t1_cfrxj\",\"author_flair_text\":null,\"author\":\"random\"}\n",
      "{\"author_flair_text\":null,\"author\":\"nzeeshan\",\"parent_id\":\"t3_fut5\",\"edited\":false,\"id\":\"cfuta\",\"gilded\":0,\"distinguished\":null,\"retrieved_on\":1473797860,\"body\":\"hiding one's shadow during a picture is very important when it comes to photography .. however in this case .. the beauty is in the shadow .. the photographer's shadow tells what stance he stood as he took the picture .. the photographer's shadow is a unique style (unintended though)\",\"controversiality\":0,\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fut5\",\"subreddit\":\"reddit.com\",\"score\":2,\"ups\":2,\"created_utc\":1157070092,\"author_flair_css_class\":null}\n",
      "{\"author_flair_text\":null,\"author\":\"axodys\",\"edited\":false,\"parent_id\":\"t3_frpe\",\"id\":\"cfutw\",\"gilded\":0,\"retrieved_on\":1473797860,\"distinguished\":null,\"body\":\"that's pretty wild from an american perspective.  it sounds somewhat similar to england a few hundred years back though.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_frpe\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157070405,\"author_flair_css_class\":null,\"ups\":1,\"score\":1}\n",
      "{\"author_flair_text\":null,\"author\":\"digital\",\"id\":\"cfuw9\",\"edited\":false,\"parent_id\":\"t3_futr\",\"gilded\":0,\"retrieved_on\":1473797862,\"distinguished\":null,\"body\":\"where's olive?\",\"controversiality\":0,\"link_id\":\"t3_futr\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"author_flair_css_class\":null,\"created_utc\":1157071310,\"score\":1,\"ups\":1}\n",
      "{\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"id\":\"cfux2\",\"edited\":false,\"parent_id\":\"t1_cfujr\",\"author_flair_text\":null,\"author\":\"oak\",\"created_utc\":1157071659,\"author_flair_css_class\":null,\"score\":3,\"ups\":3,\"subreddit\":\"reddit.com\",\"stickied\":false,\"link_id\":\"t3_fr8a\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"no secret here.\\r\\n\\r\\namericans hate who they're told to hate.\"}\n",
      "{\"edited\":false,\"id\":\"cfuyk\",\"parent_id\":\"t1_cfubd\",\"author_flair_text\":null,\"author\":\"nostrademons\",\"retrieved_on\":1473797863,\"distinguished\":null,\"gilded\":0,\"link_id\":\"t3_ftqh\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"more to the point, they were levied with war debts they simply couldn't pay, and so tried to inflate the currency to raise funds, which led to massive hyperinflation and widespread economic instability.\\r\\n\\r\\nthis makes me really worried about *our* war debts (which thankfully aren't a huge portion of the gdp *yet*, but are rapidly going in the wrong direction).  u.s. debt is denominated in dollars, which makes an inflation of the currency even more likely (unlike germany, we *could* wipe out our war debts by printing money).  that's just as bad for american citizens though, and will win us no friends abroad.\\r\\n\\r\\ni'm actually far more worried about u.s. inflation due to terrorism than i am about terrorism itself.  i suspect osama bin laden's purpose in blowing up the twin towers wasn't to tear america apart, but to make us so paranoid that we tear ourselves apart.  and he's been doing a fairly good job of it so far.\",\"author_flair_css_class\":null,\"created_utc\":1157072290,\"score\":22,\"ups\":22,\"subreddit\":\"reddit.com\"}\n"
     ]
    }
   ],
   "source": [
    "print \"\\n\".join(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Propellar', 'My Asthma Pal', 'Asthma Storylines', 'Peak Flow', 'SaniQ Asthma', 'asthmaTrack', 'Wizdy Pets - Kids asthma game', 'e-symptoms', 'AirCasting', 'Asthma Tracker', 'My Asthma App', 'Asthma Australia - Asthma App', 'KagenAir ', 'Kiss My Asthma', 'My Asthma Manager', 'Asthma Health Storylines', 'Asthmatic - the first asthma weather forecast', 'AsthmaMD', 'How To Use Inhalers for iPad', 'Breathcount | asthma control', 'Asthma Diary', 'ELFy Asthma', 'Medtep Asthma', '7 keys to manage Childhood Asthma', 'Peak Flow Manager ', 'My Breathefree', 'Asthma Test', 'Asthma Ally', 'Asthma treatment', 'Asthma Explorers Club', 'Asthma FAQ (FAQs in Asthma)', 'Peakflow', 'STAT Asthma NHLBI Guidelines', 'The Allergy, Asthma and Sinus Center', 'Asthma Logger', 'Asthma Tracker & Log Free', 'Kids Beating Asthma', 'Huff & Puff', 'mCARAT', 'Asthma Journal Free', 'Asthma Journal Pro']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "columns = defaultdict(list) # each value in each column is appended to a list\n",
    "\n",
    "with open('asthmaQualifications.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        for (i,v) in enumerate(row):\n",
    "            columns[i].append(v)\n",
    "print(columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "appnames = (columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [c.lower() for c in comments]\n",
    "appnames = [a.lower() for a in appnames]\n",
    "\n",
    "output = [c for c in comments if any(a in c for a in appnames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 53.0 failed 1 times, most recent failure: Lost task 1.0 in stage 53.0 (TID 81, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-1bc09a12926a>\", line 1, in <lambda>\nNameError: global name 'app' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-1bc09a12926a>\", line 1, in <lambda>\nNameError: global name 'app' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-1bc09a12926a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Olive'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 53.0 failed 1 times, most recent failure: Lost task 1.0 in stage 53.0 (TID 81, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-1bc09a12926a>\", line 1, in <lambda>\nNameError: global name 'app' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-140-1bc09a12926a>\", line 1, in <lambda>\nNameError: global name 'app' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd.filter(lambda x: (app in x)).collect()\n",
    "cc = rdd.filter(lambda x: ('Olive' in x))\n",
    "cc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"author_flair_text\":null,\"author\":\"axodys\",\"edited\":false,\"parent_id\":\"t3_frpe\",\"id\":\"cfutw\",\"gilded\":0,\"retrieved_on\":1473797860,\"distinguished\":null,\"body\":\"That\\'s pretty wild from an American perspective.  It sounds somewhat similar to England a few hundred years back though.\",\"controversiality\":0,\"stickied\":false,\"link_id\":\"t3_frpe\",\"subreddit_id\":\"t5_6\",\"subreddit\":\"reddit.com\",\"created_utc\":1157070405,\"author_flair_css_class\":null,\"ups\":1,\"score\":1}',\n",
       " '{\"retrieved_on\":1473797862,\"distinguished\":null,\"gilded\":0,\"id\":\"cfux2\",\"edited\":false,\"parent_id\":\"t1_cfujr\",\"author_flair_text\":null,\"author\":\"Oak\",\"created_utc\":1157071659,\"author_flair_css_class\":null,\"score\":3,\"ups\":3,\"subreddit\":\"reddit.com\",\"stickied\":false,\"link_id\":\"t3_fr8a\",\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"No secret here.\\\\r\\\\n\\\\r\\\\nAmericans hate who they\\'re told to hate.\"}',\n",
       " '{\"edited\":false,\"id\":\"cfuyk\",\"parent_id\":\"t1_cfubd\",\"author_flair_text\":null,\"author\":\"nostrademons\",\"retrieved_on\":1473797863,\"distinguished\":null,\"gilded\":0,\"link_id\":\"t3_ftqh\",\"stickied\":false,\"subreddit_id\":\"t5_6\",\"controversiality\":0,\"body\":\"More to the point, they were levied with war debts they simply couldn\\'t pay, and so tried to inflate the currency to raise funds, which led to massive hyperinflation and widespread economic instability.\\\\r\\\\n\\\\r\\\\nThis makes me really worried about *our* war debts (which thankfully aren\\'t a huge portion of the GDP *yet*, but are rapidly going in the wrong direction).  U.S. debt is denominated in dollars, which makes an inflation of the currency even more likely (unlike Germany, we *could* wipe out our war debts by printing money).  That\\'s just as bad for American citizens though, and will win us no friends abroad.\\\\r\\\\n\\\\r\\\\nI\\'m actually far more worried about U.S. inflation due to terrorism than I am about terrorism itself.  I suspect Osama bin Laden\\'s purpose in blowing up the twin towers wasn\\'t to tear America apart, but to make us so paranoid that we tear ourselves apart.  And he\\'s been doing a fairly good job of it so far.\",\"author_flair_css_class\":null,\"created_utc\":1157072290,\"score\":22,\"ups\":22,\"subreddit\":\"reddit.com\"}']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: 'American' in x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['propellar', 'my asthma pal', 'asthma storylines', 'peak flow', 'saniq asthma', 'asthmatrack', 'wizdy pets - kids asthma game', 'e-symptoms', 'aircasting', 'asthma tracker', 'my asthma app', 'asthma australia - asthma app', 'kagenair ', 'kiss my asthma', 'my asthma manager', 'asthma health storylines', 'asthmatic - the first asthma weather forecast', 'asthmamd', 'how to use inhalers for ipad', 'breathcount | asthma control', 'asthma diary', 'elfy asthma', 'medtep asthma', '7 keys to manage childhood asthma', 'peak flow manager ', 'my breathefree', 'asthma test', 'asthma ally', 'asthma treatment', 'asthma explorers club', 'asthma faq (faqs in asthma)', 'peakflow', 'stat asthma nhlbi guidelines', 'the allergy, asthma and sinus center', 'asthma logger', 'asthma tracker & log free', 'kids beating asthma', 'huff & puff', 'mcarat', 'asthma journal free', 'asthma journal pro']\n",
      "propellarmy asthma palasthma storylinespeak flowsaniq asthmaasthmatrackwizdy pets - kids asthma gamee-symptomsaircastingasthma trackermy asthma appasthma australia - asthma appkagenair kiss my asthmamy asthma managerasthma health storylinesasthmatic - the first asthma weather forecastasthmamdhow to use inhalers for ipadbreathcount | asthma controlasthma diaryelfy asthmamedtep asthma7 keys to manage childhood asthmapeak flow manager my breathefreeasthma testasthma allyasthma treatmentasthma explorers clubasthma faq (faqs in asthma)peakflowstat asthma nhlbi guidelinesthe allergy, asthma and sinus centerasthma loggerasthma tracker & log freekids beating asthmahuff & puffmcaratasthma journal freeasthma journal pro\n",
      "olive personally hiding american\n"
     ]
    }
   ],
   "source": [
    "print appnames\n",
    "apps = ''.join(appnames)\n",
    "print apps\n",
    "\n",
    "kw = ('olive personally hiding american')\n",
    "print kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComments(appnames, comments):\n",
    "    if appnames = 0:\n",
    "        comments.next()\n",
    "    import csv \n",
    "    reader = csv.reader(comments)\n",
    "    for row in reader:\n",
    "        if row[0] != 'F':\n",
    "            \n",
    "    \n",
    "def getCuisine(partId, list_of_rest):\n",
    "    count = 0\n",
    "    if partId == 0:\n",
    "        list_of_rest.next()\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_rest)\n",
    "    for row in reader:\n",
    "        if row[14]!= 'F':\n",
    "            (cuisine) = (row[7])\n",
    "            count = count + 1\n",
    "            yield (cuisine)\n",
    "\n",
    "cuisine = food.mapPartitionsWithIndex(getCuisine)\n",
    "cuisine.take(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"score\":-1,\"ups\":-1,\"created_utc\":1157068927,\"author_flair_css_class\":null,\"subreddit\":\"reddit.com\",\"subreddit_id\":\"t5_6\",\"stickied\":false,\"link_id\":\"t3_fr9p\",\"body\":\"This is untested territory legally - I can\\'t wait to see what happens to the first entity who exploits someone\\'s uploaded work in a way the creator doesn\\'t like and/or makes loads of money from it and doesn\\'t pay the creator anything. To give them the benefit of the doubt, I think most \\\\\"sharing\\\\\" site included scary-sounding ToS to protect themselves from some uploader being a jerk: \\\\\"Hey, there\\'s an ad on the same page as my stuff, I never agreed to that, I\\'m suing you!\\\\\"\\\\r\\\\n\\\\r\\\\nArguably, these sites have \\\\\"paid\\\\\" their contributors by doing unpaid promotion work and providing hosting.  Anyone have any idea how much it would cost an individual to host and promote a video such that it reaches millions of people?  I think one of these sites will cross a line - e.g. put out a CD/DVD that makes millions and pay nothing to the contributors.  Then we\\'ll see what happens as lawyers see an opportunity to chase big bucks through the courts.\",\"controversiality\":0,\"distinguished\":null,\"retrieved_on\":1473797859,\"gilded\":0,\"id\":\"cfuqz\",\"edited\":false,\"parent_id\":\"t3_fr9p\",\"author_flair_text\":null,\"author\":\"Anderkay\"}']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2331.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2331.0 (TID 4599, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-231-8e3b138d2ace>\", line 7, in <lambda>\nTypeError: string indices must be integers, not str\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-231-8e3b138d2ace>\", line 7, in <lambda>\nTypeError: string indices must be integers, not str\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-8e3b138d2ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrdd4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrdd4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#filt = list(filter(lambda x: any(s in x.lower() for s in stopwords),comments))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print filt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2331.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2331.0 (TID 4599, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-231-8e3b138d2ace>\", line 7, in <lambda>\nTypeError: string indices must be integers, not str\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/julialau/spark/spark-2.3.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-231-8e3b138d2ace>\", line 7, in <lambda>\nTypeError: string indices must be integers, not str\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['Olive','American']\n",
    "#output = [c for c in comments if any(a in c for a in appnames)]\n",
    "#filt = list(filter(lambda x: not any(s in x.lower() for s in stopwords),comments))\n",
    "#rdd3 = rdd.filter(lambda x: any(s in x.lower() for s in stopwords))\n",
    "#rdd3.take(5)\n",
    "\n",
    "rdd4 = rdd.filter(lambda x: x not in stopwords)\n",
    "rdd4.take(1000)\n",
    "#filt = list(filter(lambda x: any(s in x.lower() for s in stopwords),comments))#print filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['created_utc:1157068946',\n",
       " 'created_utc:1157068993',\n",
       " 'subreddit_id:\"t5_6\"',\n",
       " 'subreddit:\"programming\"',\n",
       " 'score:12',\n",
       " 'body:\"He was probably to stubborn to turn back... that would be a bit embarassing...\"',\n",
       " 'created_utc:1157069314',\n",
       " 'created_utc:1157069566',\n",
       " 'controversiality:0',\n",
       " 'controversiality:0',\n",
       " 'body:\"Personally',\n",
       " 'score:4',\n",
       " 'ups:2',\n",
       " 'body:\"Hiding one\\'s shadow during a picture is very important when it comes to photography .. however in this case .. the beauty is in the shadow .. the photographer\\'s shadow tells what stance he stood as he took the picture .. the photographer\\'s shadow is a unique style (unintended though)\"',\n",
       " 'created_utc:1157070134',\n",
       " 'score:1',\n",
       " 'body:\"Powers of 10 is a film dealing with the relative size of things in the Universe',\n",
       " 'stickied:false',\n",
       " 'body:\"There\\'s a common misconception among people that everything looks black and white to colorblind people .. well it\\'s not true (only in total color blindness you see black and white). There are many different types of degress when it comes to color deficiency .. following images show you how colors change to a color blind person ..\"',\n",
       " 'link_id:\"t3_futq\"',\n",
       " 'body:\"That\\'s pretty wild from an American perspective.  It sounds somewhat similar to England a few hundred years back though.\"',\n",
       " 'controversiality:0',\n",
       " 'body:\"\\\\\"As for sensing pain',\n",
       " 'author_flair_css_class:null',\n",
       " 'ups:2',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'ups:4',\n",
       " 'body:\"*Yes',\n",
       " 'subreddit_id:\"t5_6\"',\n",
       " 'score:11',\n",
       " 'stickied:false',\n",
       " 'ups:1',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'controversiality:1',\n",
       " 'link_id:\"t3_ftqh\"',\n",
       " 'body:\"Where\\'s Olive?\"',\n",
       " 'controversiality:0',\n",
       " 'controversiality:0',\n",
       " 'created_utc:1157071471',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'score:1',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'stickied:false',\n",
       " 'link_id:\"t3_fumz\"',\n",
       " 'created_utc:1157071659',\n",
       " 'subreddit_id:\"t5_6\"',\n",
       " 'score:-1',\n",
       " 'body:\"Before you post the real explanation',\n",
       " ' I bet she\\'d go for the women-only car - but she shouldn\\'t have to make that choice!  Why can\\'t we have unsegregated cars where people respect each other?\"',\n",
       " 'score:14',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'subreddit_id:\"t5_6\"',\n",
       " 'link_id:\"t3_ftqh\"',\n",
       " 'score:-6',\n",
       " 'author_flair_css_class:null',\n",
       " 'subreddit:\"programming\"',\n",
       " ' which are very hard to pin down.  Psychology is not a strict science and mis-diagnosis is extremely common.\\\\r\\\\n\\\\r\\\\nOn the other hand',\n",
       " 'body:\"right',\n",
       " ' the dude in the link is arguing about the convenience',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'ups:1',\n",
       " 'subreddit_id:\"t5_6\"',\n",
       " 'controversiality:0',\n",
       " ' aspects-oriented programming is a lot more function-oriented and could even simplify OOP implementations in JS [2].\\\\r\\\\n\\\\r\\\\n[1] Lua has proven that it works for scripting as well.\\\\r\\\\n\\\\r\\\\n[2] I envision a future where functions (which are objects) are pointcuts and join points are `Function.prototype` methods and you can do neat things like `alert.after += function {...}`.\"',\n",
       " 'controversiality:0',\n",
       " 'author_flair_css_class:null',\n",
       " 'body:\"None of this is a new language feature. This is a technique building upon delegates using the fact that you can create an array of delegates (or any type) in C#. This is no different than creating an array of (first-class) functions and iterating over them',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'body:\"http://www.memritv.org/search.asp?ACT=S9&amp;P1=941\\\\r\\\\n\\\\r\\\\nHey',\n",
       " 'author_flair_css_class:null',\n",
       " 'subreddit_id:\"t5_6\"',\n",
       " 'body:\"[deleted]\"',\n",
       " 'stickied:false',\n",
       " 'author_flair_css_class:null',\n",
       " 'stickied:false',\n",
       " ' an apostle from the Bible',\n",
       " 'controversiality:0',\n",
       " 'ups:4',\n",
       " 'subreddit:\"programming\"',\n",
       " 'stickied:false',\n",
       " 'stickied:false',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'stickied:false',\n",
       " 'score:1',\n",
       " 'body:\"You say that the climate model is \\\\\"tenuous at best\\\\\"',\n",
       " 'author_flair_css_class:null',\n",
       " 'controversiality:0',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'created_utc:1157074429',\n",
       " 'controversiality:0',\n",
       " 'subreddit:\"reddit.com\"',\n",
       " 'stickied:false',\n",
       " 'author_flair_css_class:null',\n",
       " 'created_utc:1157074585',\n",
       " 'ups:-1',\n",
       " 'subreddit_id:\"t5_2fwo\"',\n",
       " 'link_id:\"t3_ftam\"']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def getBody(partId, list_of_comments):\n",
    "    if partId == 0:\n",
    "        list_of_comments.next()\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_comments)\n",
    "    for row in reader:\n",
    "        if row[8] != 'body':\n",
    "            (body) = (row[8])\n",
    "            yield(body)\n",
    "        \n",
    "    body = rdd.mapPartitionsWithIndex(getBody)\n",
    "body.take(101)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body:\"He was probably to stubborn to turn back... that would be a bit embarassing...\"',\n",
       " 'body:\"Personally',\n",
       " 'body:\"Hiding one\\'s shadow during a picture is very important when it comes to photography .. however in this case .. the beauty is in the shadow .. the photographer\\'s shadow tells what stance he stood as he took the picture .. the photographer\\'s shadow is a unique style (unintended though)\"',\n",
       " 'body:\"Powers of 10 is a film dealing with the relative size of things in the Universe',\n",
       " 'body:\"There\\'s a common misconception among people that everything looks black and white to colorblind people .. well it\\'s not true (only in total color blindness you see black and white). There are many different types of degress when it comes to color deficiency .. following images show you how colors change to a color blind person ..\"',\n",
       " 'body:\"That\\'s pretty wild from an American perspective.  It sounds somewhat similar to England a few hundred years back though.\"',\n",
       " 'body:\"\\\\\"As for sensing pain',\n",
       " 'body:\"*Yes',\n",
       " 'body:\"Where\\'s Olive?\"',\n",
       " 'body:\"Before you post the real explanation',\n",
       " 'body:\"right',\n",
       " 'body:\"None of this is a new language feature. This is a technique building upon delegates using the fact that you can create an array of delegates (or any type) in C#. This is no different than creating an array of (first-class) functions and iterating over them',\n",
       " 'body:\"http://www.memritv.org/search.asp?ACT=S9&amp;P1=941\\\\r\\\\n\\\\r\\\\nHey',\n",
       " 'body:\"[deleted]\"',\n",
       " 'body:\"You say that the climate model is \\\\\"tenuous at best\\\\\"']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body.filter(lambda x: 'body' in x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualbody = body.filter(lambda x: 'body' in x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body:\"He was probably to stubborn to turn back... that would be a bit embarassing...\"', 'body:\"Personally', 'body:\"Hiding one\\'s shadow during a picture is very important when it comes to photography .. however in this case .. the beauty is in the shadow .. the photographer\\'s shadow tells what stance he stood as he took the picture .. the photographer\\'s shadow is a unique style (unintended though)\"', 'body:\"Powers of 10 is a film dealing with the relative size of things in the Universe', 'body:\"There\\'s a common misconception among people that everything looks black and white to colorblind people .. well it\\'s not true (only in total color blindness you see black and white). There are many different types of degress when it comes to color deficiency .. following images show you how colors change to a color blind person ..\"', 'body:\"That\\'s pretty wild from an American perspective.  It sounds somewhat similar to England a few hundred years back though.\"', 'body:\"\\\\\"As for sensing pain', 'body:\"*Yes', 'body:\"Where\\'s Olive?\"', 'body:\"Before you post the real explanation', 'body:\"right', 'body:\"None of this is a new language feature. This is a technique building upon delegates using the fact that you can create an array of delegates (or any type) in C#. This is no different than creating an array of (first-class) functions and iterating over them', 'body:\"http://www.memritv.org/search.asp?ACT=S9&amp;P1=941\\\\r\\\\n\\\\r\\\\nHey', 'body:\"[deleted]\"', 'body:\"You say that the climate model is \\\\\"tenuous at best\\\\\"']\n"
     ]
    }
   ],
   "source": [
    "print actualbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-837c81ab66c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactualbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactualbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "actualbody = ''.join(actualbody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method lower of str object at 0x7f9bbe184e00>\n"
     ]
    }
   ],
   "source": [
    "print actualbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "appnames = [a.lower() for a in appnames]\n",
    "keywords = ['Olive', 'Personally', 'Hiding', 'American']\n",
    "\n",
    "output = [b for b in actualbody if any (k in b for k in keywords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'o', 'd', 'y', ':', '\"', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'p', 'r', 'o', 'b', 'a', 'b', 'l', 'y', ' ', 't', 'o', ' ', 's', 't', 'u', 'b', 'b', 'o', 'r', 'n', ' ', 't', 'o', ' ', 't', 'u', 'r', 'n', ' ', 'b', 'a', 'c', 'k', '.', '.', '.', ' ', 't', 'h', 'a', 't', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'a', ' ', 'b', 'i', 't', ' ', 'e', 'm', 'b', 'a', 'r', 'a', 's', 's', 'i', 'n', 'g', '.', '.', '.', '\"', 'b', 'o', 'd', 'y', ':', '\"', 'p', 'e', 'r', 's', 'o', 'n', 'a', 'l', 'l', 'y', 'b', 'o', 'd', 'y', ':', '\"', 'h', 'i', 'd', 'i', 'n', 'g', ' ', 'o', 'n', 'e', \"'\", 's', ' ', 's', 'h', 'a', 'd', 'o', 'w', ' ', 'd', 'u', 'r', 'i', 'n', 'g', ' ', 'a', ' ', 'p', 'i', 'c', 't', 'u', 'r', 'e', ' ', 'i', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 'c', 'o', 'm', 'e', 's', ' ', 't', 'o', ' ', 'p', 'h', 'o', 't', 'o', 'g', 'r', 'a', 'p', 'h', 'y', ' ', '.', '.', ' ', 'h', 'o', 'w', 'e', 'v', 'e', 'r', ' ', 'i', 'n', ' ', 't', 'h', 'i', 's', ' ', 'c', 'a', 's', 'e', ' ', '.', '.', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'a', 'u', 't', 'y', ' ', 'i', 's', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 's', 'h', 'a', 'd', 'o', 'w', ' ', '.', '.', ' ', 't', 'h', 'e', ' ', 'p', 'h', 'o', 't', 'o', 'g', 'r', 'a', 'p', 'h', 'e', 'r', \"'\", 's', ' ', 's', 'h', 'a', 'd', 'o', 'w', ' ', 't', 'e', 'l', 'l', 's', ' ', 'w', 'h', 'a', 't', ' ', 's', 't', 'a', 'n', 'c', 'e', ' ', 'h', 'e', ' ', 's', 't', 'o', 'o', 'd', ' ', 'a', 's', ' ', 'h', 'e', ' ', 't', 'o', 'o', 'k', ' ', 't', 'h', 'e', ' ', 'p', 'i', 'c', 't', 'u', 'r', 'e', ' ', '.', '.', ' ', 't', 'h', 'e', ' ', 'p', 'h', 'o', 't', 'o', 'g', 'r', 'a', 'p', 'h', 'e', 'r', \"'\", 's', ' ', 's', 'h', 'a', 'd', 'o', 'w', ' ', 'i', 's', ' ', 'a', ' ', 'u', 'n', 'i', 'q', 'u', 'e', ' ', 's', 't', 'y', 'l', 'e', ' ', '(', 'u', 'n', 'i', 'n', 't', 'e', 'n', 'd', 'e', 'd', ' ', 't', 'h', 'o', 'u', 'g', 'h', ')', '\"', 'b', 'o', 'd', 'y', ':', '\"', 'p', 'o', 'w', 'e', 'r', 's', ' ', 'o', 'f', ' ', '1', '0', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'i', 'l', 'm', ' ', 'd', 'e', 'a', 'l', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'v', 'e', ' ', 's', 'i', 'z', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'e', 'b', 'o', 'd', 'y', ':', '\"', 't', 'h', 'e', 'r', 'e', \"'\", 's', ' ', 'a', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 'm', 'i', 's', 'c', 'o', 'n', 'c', 'e', 'p', 't', 'i', 'o', 'n', ' ', 'a', 'm', 'o', 'n', 'g', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 't', 'h', 'a', 't', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 'l', 'o', 'o', 'k', 's', ' ', 'b', 'l', 'a', 'c', 'k', ' ', 'a', 'n', 'd', ' ', 'w', 'h', 'i', 't', 'e', ' ', 't', 'o', ' ', 'c', 'o', 'l', 'o', 'r', 'b', 'l', 'i', 'n', 'd', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', '.', '.', ' ', 'w', 'e', 'l', 'l', ' ', 'i', 't', \"'\", 's', ' ', 'n', 'o', 't', ' ', 't', 'r', 'u', 'e', ' ', '(', 'o', 'n', 'l', 'y', ' ', 'i', 'n', ' ', 't', 'o', 't', 'a', 'l', ' ', 'c', 'o', 'l', 'o', 'r', ' ', 'b', 'l', 'i', 'n', 'd', 'n', 'e', 's', 's', ' ', 'y', 'o', 'u', ' ', 's', 'e', 'e', ' ', 'b', 'l', 'a', 'c', 'k', ' ', 'a', 'n', 'd', ' ', 'w', 'h', 'i', 't', 'e', ')', '.', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 'm', 'a', 'n', 'y', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 't', 'y', 'p', 'e', 's', ' ', 'o', 'f', ' ', 'd', 'e', 'g', 'r', 'e', 's', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'i', 't', ' ', 'c', 'o', 'm', 'e', 's', ' ', 't', 'o', ' ', 'c', 'o', 'l', 'o', 'r', ' ', 'd', 'e', 'f', 'i', 'c', 'i', 'e', 'n', 'c', 'y', ' ', '.', '.', ' ', 'f', 'o', 'l', 'l', 'o', 'w', 'i', 'n', 'g', ' ', 'i', 'm', 'a', 'g', 'e', 's', ' ', 's', 'h', 'o', 'w', ' ', 'y', 'o', 'u', ' ', 'h', 'o', 'w', ' ', 'c', 'o', 'l', 'o', 'r', 's', ' ', 'c', 'h', 'a', 'n', 'g', 'e', ' ', 't', 'o', ' ', 'a', ' ', 'c', 'o', 'l', 'o', 'r', ' ', 'b', 'l', 'i', 'n', 'd', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ', '.', '.', '\"', 'b', 'o', 'd', 'y', ':', '\"', 't', 'h', 'a', 't', \"'\", 's', ' ', 'p', 'r', 'e', 't', 't', 'y', ' ', 'w', 'i', 'l', 'd', ' ', 'f', 'r', 'o', 'm', ' ', 'a', 'n', ' ', 'a', 'm', 'e', 'r', 'i', 'c', 'a', 'n', ' ', 'p', 'e', 'r', 's', 'p', 'e', 'c', 't', 'i', 'v', 'e', '.', ' ', ' ', 'i', 't', ' ', 's', 'o', 'u', 'n', 'd', 's', ' ', 's', 'o', 'm', 'e', 'w', 'h', 'a', 't', ' ', 's', 'i', 'm', 'i', 'l', 'a', 'r', ' ', 't', 'o', ' ', 'e', 'n', 'g', 'l', 'a', 'n', 'd', ' ', 'a', ' ', 'f', 'e', 'w', ' ', 'h', 'u', 'n', 'd', 'r', 'e', 'd', ' ', 'y', 'e', 'a', 'r', 's', ' ', 'b', 'a', 'c', 'k', ' ', 't', 'h', 'o', 'u', 'g', 'h', '.', '\"', 'b', 'o', 'd', 'y', ':', '\"', '\\\\', '\"', 'a', 's', ' ', 'f', 'o', 'r', ' ', 's', 'e', 'n', 's', 'i', 'n', 'g', ' ', 'p', 'a', 'i', 'n', 'b', 'o', 'd', 'y', ':', '\"', '*', 'y', 'e', 's', 'b', 'o', 'd', 'y', ':', '\"', 'w', 'h', 'e', 'r', 'e', \"'\", 's', ' ', 'o', 'l', 'i', 'v', 'e', '?', '\"', 'b', 'o', 'd', 'y', ':', '\"', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'y', 'o', 'u', ' ', 'p', 'o', 's', 't', ' ', 't', 'h', 'e', ' ', 'r', 'e', 'a', 'l', ' ', 'e', 'x', 'p', 'l', 'a', 'n', 'a', 't', 'i', 'o', 'n', 'b', 'o', 'd', 'y', ':', '\"', 'r', 'i', 'g', 'h', 't', 'b', 'o', 'd', 'y', ':', '\"', 'n', 'o', 'n', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 'n', 'e', 'w', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', '.', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 't', 'e', 'c', 'h', 'n', 'i', 'q', 'u', 'e', ' ', 'b', 'u', 'i', 'l', 'd', 'i', 'n', 'g', ' ', 'u', 'p', 'o', 'n', ' ', 'd', 'e', 'l', 'e', 'g', 'a', 't', 'e', 's', ' ', 'u', 's', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'f', 'a', 'c', 't', ' ', 't', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'c', 'r', 'e', 'a', 't', 'e', ' ', 'a', 'n', ' ', 'a', 'r', 'r', 'a', 'y', ' ', 'o', 'f', ' ', 'd', 'e', 'l', 'e', 'g', 'a', 't', 'e', 's', ' ', '(', 'o', 'r', ' ', 'a', 'n', 'y', ' ', 't', 'y', 'p', 'e', ')', ' ', 'i', 'n', ' ', 'c', '#', '.', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'n', 'o', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 't', 'h', 'a', 'n', ' ', 'c', 'r', 'e', 'a', 't', 'i', 'n', 'g', ' ', 'a', 'n', ' ', 'a', 'r', 'r', 'a', 'y', ' ', 'o', 'f', ' ', '(', 'f', 'i', 'r', 's', 't', '-', 'c', 'l', 'a', 's', 's', ')', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 's', ' ', 'a', 'n', 'd', ' ', 'i', 't', 'e', 'r', 'a', 't', 'i', 'n', 'g', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', 'm', 'b', 'o', 'd', 'y', ':', '\"', 'h', 't', 't', 'p', ':', '/', '/', 'w', 'w', 'w', '.', 'm', 'e', 'm', 'r', 'i', 't', 'v', '.', 'o', 'r', 'g', '/', 's', 'e', 'a', 'r', 'c', 'h', '.', 'a', 's', 'p', '?', 'a', 'c', 't', '=', 's', '9', '&', 'a', 'm', 'p', ';', 'p', '1', '=', '9', '4', '1', '\\\\', 'r', '\\\\', 'n', '\\\\', 'r', '\\\\', 'n', 'h', 'e', 'y', 'b', 'o', 'd', 'y', ':', '\"', '[', 'd', 'e', 'l', 'e', 't', 'e', 'd', ']', '\"', 'b', 'o', 'd', 'y', ':', '\"', 'y', 'o', 'u', ' ', 's', 'a', 'y', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'c', 'l', 'i', 'm', 'a', 't', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'i', 's', ' ', '\\\\', '\"', 't', 'e', 'n', 'u', 'o', 'u', 's', ' ', 'a', 't', ' ', 'b', 'e', 's', 't', '\\\\', '\"']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
